# ===========================================
# server/app/services/performance_service.py
# Config ì„¤ì •ì„ í™œìš©í•œ ì™„ì „í•œ ìºì‹± ë²„ì „
# ===========================================

import pandas as pd
import yfinance as yf
from pykrx import stock
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import time
import json
import hashlib
from functools import lru_cache

# âœ… Config ì„¤ì • import ì¶”ê°€
from app.config import settings, get_redis_url

# Redis ìºì‹± (ì„ íƒì‚¬í•­)
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

logger = logging.getLogger(__name__)

class CacheService:
    """Config ê¸°ë°˜ Redis ìºì‹± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.enabled = False
        self.redis_client = None
        self.memory_cache = {}
        self.cache_timestamps = {}
        
        # âœ… Config ê¸°ë°˜ Redis ì´ˆê¸°í™”
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(
                    host=settings.REDIS_HOST,
                    port=settings.REDIS_PORT,
                    db=settings.REDIS_DB,
                    password=settings.REDIS_PASSWORD if settings.REDIS_PASSWORD else None,
                    decode_responses=True,
                    socket_timeout=1,
                    socket_connect_timeout=1
                )
                
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                self.redis_client.ping()
                self.enabled = True
                logger.info(f"âœ… Redis ìºì‹œ ì—°ê²° ì„±ê³µ: {settings.REDIS_HOST}:{settings.REDIS_PORT}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨ ({settings.REDIS_HOST}:{settings.REDIS_PORT}), ë©”ëª¨ë¦¬ ìºì‹± ì‚¬ìš©: {e}")
                self.redis_client = None
                self.enabled = False
        else:
            logger.info("âš ï¸ Redis ë¯¸ì„¤ì¹˜, ë©”ëª¨ë¦¬ ìºì‹± ì‚¬ìš©")
    
    def _generate_key(self, prefix: str, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = json.dumps(kwargs, sort_keys=True)
        key_hash = hashlib.md5(key_data.encode()).hexdigest()[:8]
        return f"{prefix}:{key_hash}"
    
    def get(self, prefix: str, **kwargs) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        key = self._generate_key(prefix, **kwargs)
        
        # Redis ìš°ì„  ì‹œë„
        if self.enabled:
            try:
                cached_data = self.redis_client.get(key)
                if cached_data:
                    return json.loads(cached_data)
            except Exception as e:
                logger.warning(f"Redis ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            timestamp = self.cache_timestamps.get(key, 0)
            if time.time() - timestamp < settings.PERFORMANCE_CACHE_TTL:
                return self.memory_cache[key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self.memory_cache[key]
                del self.cache_timestamps[key]
        
        return None
    
    def set(self, prefix: str, data: Any, ttl: int = None, **kwargs) -> bool:
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        if ttl is None:
            ttl = settings.PERFORMANCE_CACHE_TTL
            
        key = self._generate_key(prefix, **kwargs)
        
        # Redisì— ì €ì¥ ì‹œë„
        if self.enabled:
            try:
                self.redis_client.setex(key, ttl, json.dumps(data))
                return True
            except Exception as e:
                logger.warning(f"Redis ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self.memory_cache[key] = data
        self.cache_timestamps[key] = time.time()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
        if len(self.memory_cache) > 100:
            oldest_key = min(self.cache_timestamps.keys(), 
                           key=lambda k: self.cache_timestamps[k])
            del self.memory_cache[oldest_key]
            del self.cache_timestamps[oldest_key]
        
        return True

    def clear(self, pattern: str = None) -> bool:
        """ìºì‹œ í´ë¦¬ì–´"""
        try:
            if self.enabled and self.redis_client:
                if pattern:
                    keys = self.redis_client.keys(pattern)
                    if keys:
                        self.redis_client.delete(*keys)
                        logger.info(f"Redis íŒ¨í„´ ìºì‹œ í´ë¦¬ì–´: {pattern} ({len(keys)}ê°œ í‚¤)")
                else:
                    keys = self.redis_client.keys("market_performance:*")
                    if keys:
                        self.redis_client.delete(*keys)
                        logger.info(f"Redis ì „ì²´ ìºì‹œ í´ë¦¬ì–´: {len(keys)}ê°œ í‚¤")
            
            # ë©”ëª¨ë¦¬ ìºì‹œë„ í´ë¦¬ì–´
            if pattern:
                # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì„ íƒì  ì‚­ì œ
                keys_to_delete = [k for k in self.memory_cache.keys() if pattern.replace('*', '') in k]
                for k in keys_to_delete:
                    del self.memory_cache[k]
                    del self.cache_timestamps[k]
            else:
                self.memory_cache.clear()
                self.cache_timestamps.clear()
            
            return True
        except Exception as e:
            logger.error(f"ìºì‹œ í´ë¦¬ì–´ ì‹¤íŒ¨: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        stats = {
            "redis_enabled": self.enabled,
            "redis_host": f"{settings.REDIS_HOST}:{settings.REDIS_PORT}",
            "memory_cache_size": len(self.memory_cache),
            "redis_keys": 0,
            "cache_ttl": settings.PERFORMANCE_CACHE_TTL
        }
        
        try:
            if self.enabled and self.redis_client:
                keys = self.redis_client.keys("market_performance:*")
                stats["redis_keys"] = len(keys)
        except Exception as e:
            logger.warning(f"Redis í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return stats

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
cache_service = CacheService()

class PerformanceService:
    """Config ê¸°ë°˜ ì„±ê³¼ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # âœ… Configì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
        self.max_workers = 4
        self.chunk_size = settings.PERFORMANCE_CHUNK_SIZE
        self.timeout = settings.PERFORMANCE_TIMEOUT
        self.max_tickers = settings.PERFORMANCE_MAX_TICKERS
        self.max_chunks = settings.PERFORMANCE_MAX_CHUNKS
        self.cache_ttl = settings.PERFORMANCE_CACHE_TTL
        
        logger.info(f"ğŸ”§ PerformanceService ì´ˆê¸°í™”: "
                   f"chunk_size={self.chunk_size}, timeout={self.timeout}s, "
                   f"max_tickers={self.max_tickers}, cache_ttl={self.cache_ttl}s")
        
    @lru_cache(maxsize=100)
    def _get_tickers_by_market_cached(self, market: str) -> pd.DataFrame:
        """ìºì‹±ëœ í‹°ì»¤ ëª©ë¡ ì¡°íšŒ"""
        return self._get_tickers_by_market(market)
    
    def _get_tickers_by_market(self, market: str) -> pd.DataFrame:
        """ì‹œì¥ì˜ í‹°ì»¤ì™€ ì´ë¦„ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            if market in ["KOSPI", "KOSDAQ"]:
                today_str = datetime.now().strftime('%Y%m%d')
                tickers = stock.get_market_ticker_list(market=market, date=today_str)
                valid_tickers = [t for t in tickers if len(t) == 6 and t.isdigit()]
                names = [stock.get_market_ticker_name(t) for t in valid_tickers]
                df = pd.DataFrame({'ticker': valid_tickers, 'name': names})
            elif market in ['NASDAQ', 'NYSE', 'S&P500']:
                try:
                    import FinanceDataReader as fdr
                    df = fdr.StockListing(market)
                    df = df[['Symbol', 'Name']].rename(columns={'Symbol': 'ticker', 'Name': 'name'})
                except ImportError:
                    logger.error("FinanceDataReaderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    df = pd.DataFrame()
            else:
                df = pd.DataFrame()
            return df
        except Exception as e:
            logger.error(f"'{market}' í‹°ì»¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _get_performance_kr(self, tickers: List[str], name_map: Dict[str, str], 
                           start_date: str, end_date: str) -> pd.DataFrame:
        """í•œêµ­ ì£¼ì‹ì˜ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        performance_data = []
        
        # Config ê¸°ë°˜ ì¢…ëª© ìˆ˜ ì œí•œ
        limited_tickers = tickers[:min(500, self.max_tickers // 2)]
        
        logger.info(f"í•œêµ­ ì£¼ì‹ ë¶„ì„ ì‹œì‘: {len(limited_tickers)}ê°œ ì¢…ëª©")
        
        for i, ticker in enumerate(limited_tickers):
            try:
                # ì§„í–‰ë¥  ë¡œê¹…
                if i % (len(limited_tickers) // 10 + 1) == 0:
                    progress = (i / len(limited_tickers)) * 100
                    logger.info(f"í•œêµ­ ì£¼ì‹ ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(limited_tickers)})")
                
                df = stock.get_market_ohlcv(
                    start_date.replace('-', ''), 
                    end_date.replace('-', ''), 
                    ticker
                )
                
                if not df.empty and len(df) > 1:
                    start_price = df['ì¢…ê°€'].iloc[0]
                    end_price = df['ì¢…ê°€'].iloc[-1]
                    
                    if start_price > 0:
                        performance = ((end_price - start_price) / start_price) * 100
                        
                        # ì´ìƒì¹˜ í•„í„°ë§
                        if -90 <= performance <= 900:
                            performance_data.append({
                                "ticker": ticker,
                                "name": name_map.get(ticker, ticker),
                                "performance": performance
                            })
                
                # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                if i % 10 == 0:
                    time.sleep(0.1)
                    
            except Exception as e:
                logger.warning(f"í•œêµ­ ì£¼ì‹ {ticker} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"í•œêµ­ ì£¼ì‹ ë¶„ì„ ì™„ë£Œ: {len(performance_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘")
        return pd.DataFrame(performance_data)

    def _get_performance_us_optimized(self, tickers: List[str], name_map: Dict[str, str], 
                                    start_date: str, end_date: str) -> pd.DataFrame:
        """Config ê¸°ë°˜ ìµœì í™”ëœ ë¯¸êµ­ ì£¼ì‹ ì„±ëŠ¥ ë¶„ì„"""
        all_performance_data = []
        
        # Config ê¸°ë°˜ í‹°ì»¤ ìˆ˜ ì œí•œ
        limited_tickers = tickers[:self.max_tickers]
        
        logger.info(f"ë¯¸êµ­ ì£¼ì‹ ë¶„ì„ ì‹œì‘: {len(limited_tickers)}ê°œ ì¢…ëª© (ì›ë³¸: {len(tickers)}ê°œ)")
        
        processed_chunks = 0
        
        for i in range(0, len(limited_tickers), self.chunk_size):
            if processed_chunks >= self.max_chunks:
                logger.warning(f"ì²­í¬ ì œí•œ ë„ë‹¬ ({self.max_chunks}ê°œ), ë¶„ì„ ì¤‘ë‹¨")
                break
                
            chunk = limited_tickers[i:i + self.chunk_size]
            
            try:
                # Config ê¸°ë°˜ ì¬ì‹œë„ ë¡œì§
                data = None
                for attempt in range(2):
                    try:
                        data = yf.download(
                            chunk, 
                            start=start_date, 
                            end=end_date, 
                            progress=False, 
                            auto_adjust=True,
                            timeout=self.timeout
                        )
                        break
                    except Exception as e:
                        if attempt == 1:
                            logger.warning(f"ì²­í¬ {i}-{i+self.chunk_size} ìµœì¢… ì‹¤íŒ¨: {e}")
                            break
                        else:
                            time.sleep(1)
                
                if data is None or data.empty or 'Close' not in data.columns:
                    continue

                close_prices = data['Close'].dropna(axis=1, how='all')
                if close_prices.empty or len(close_prices) < 2:
                    continue

                start_prices = close_prices.bfill().iloc[0]
                end_prices = close_prices.ffill().iloc[-1]
                
                perf_series = ((end_prices - start_prices) / start_prices) * 100
                
                for ticker, performance in perf_series.items():
                    if pd.notna(performance) and -90 <= performance <= 900:
                        all_performance_data.append({
                            "ticker": ticker,
                            "name": name_map.get(ticker, ticker),
                            "performance": performance
                        })

                processed_chunks += 1
                
                # ì§„í–‰ë¥  ë¡œê¹…
                progress = (processed_chunks / min(self.max_chunks, len(limited_tickers) // self.chunk_size + 1)) * 100
                logger.info(f"ë¯¸êµ­ ì£¼ì‹ ì§„í–‰ë¥ : {progress:.1f}% ({processed_chunks} ì²­í¬ ì™„ë£Œ)")
                
                # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                time.sleep(0.2)
                
            except Exception as e:
                logger.warning(f"ì²­í¬ {i}-{i+self.chunk_size} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ë¯¸êµ­ ì£¼ì‹ ë¶„ì„ ì™„ë£Œ: {len(all_performance_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘")
        return pd.DataFrame(all_performance_data)

    def _get_performance_us(self, tickers: List[str], name_map: Dict[str, str], 
                           start_date: str, end_date: str) -> pd.DataFrame:
        """í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­"""
        return self._get_performance_us_optimized(tickers, name_map, start_date, end_date)

    def get_market_performance(self, market: str, start_date: str, end_date: str, top_n: int) -> Dict[str, Any]:
        """Config ê¸°ë°˜ ì‹œì¥ ì„±ê³¼ ë¶„ì„ - ìºì‹± ì ìš©"""
        logger.info(f"ì„±ëŠ¥ ë¶„ì„ ì‹œì‘: Market={market}, Period={start_date}~{end_date}, Top={top_n}")
        
        # ìºì‹œ í™•ì¸
        cached_result = cache_service.get(
            "market_performance",
            market=market,
            start_date=start_date,
            end_date=end_date,
            top_n=top_n
        )
        
        if cached_result:
            logger.info(f"âœ… ìºì‹œì—ì„œ ë°ì´í„° ë°˜í™˜: {market}")
            return cached_result
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê³„ì‚° ìˆ˜í–‰
        listing = self._get_tickers_by_market_cached(market)
        if listing.empty:
            logger.warning(f"'{market}'ì— ëŒ€í•œ í‹°ì»¤ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"top_performers": [], "bottom_performers": []}

        tickers = listing['ticker'].tolist()
        name_map = listing.set_index('ticker')['name'].to_dict()

        # ì‹œì¥ë³„ ì„±ëŠ¥ ë¶„ì„
        if market in ["KOSPI", "KOSDAQ"]:
            performance_df = self._get_performance_kr(tickers, name_map, start_date, end_date)
        else:
            performance_df = self._get_performance_us_optimized(tickers, name_map, start_date, end_date)
        
        if performance_df.empty:
            result = {"top_performers": [], "bottom_performers": []}
        else:
            # ì •ë ¬ ë° ê²°ê³¼ ìƒì„±
            performance_df = performance_df.sort_values(by='performance', ascending=False)
            
            top_performers = performance_df.head(top_n).to_dict('records')
            bottom_performers = performance_df.tail(top_n).sort_values(
                by='performance', ascending=True
            ).to_dict('records')
            
            result = {
                "top_performers": top_performers, 
                "bottom_performers": bottom_performers,
                "total_analyzed": len(performance_df),
                "analysis_period": f"{start_date} ~ {end_date}",
                "market": market
            }
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        cache_service.set(
            "market_performance",
            result,
            ttl=self.cache_ttl,
            market=market,
            start_date=start_date,
            end_date=end_date,
            top_n=top_n
        )
        
        logger.info(f"ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ: ìƒìœ„ {len(result.get('top_performers', []))}ê°œ, "
                   f"í•˜ìœ„ {len(result.get('bottom_performers', []))}ê°œ")
        
        return result

    def get_market_performance_fast(self, market: str, start_date: str, end_date: str, top_n: int = 20) -> Dict[str, Any]:
        """ë¹ ë¥¸ ì„±ê³¼ ë¶„ì„ (ë” ì œí•œì )"""
        original_max_tickers = self.max_tickers
        original_chunk_size = self.chunk_size
        original_max_chunks = self.max_chunks
        
        try:
            # ë¹ ë¥¸ ë¶„ì„ìš© ì œí•œ ì„¤ì •
            self.max_tickers = 500
            self.chunk_size = 25
            self.max_chunks = 10
            
            return self.get_market_performance(market, start_date, end_date, min(top_n, 20))
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self.max_tickers = original_max_tickers
            self.chunk_size = original_chunk_size
            self.max_chunks = original_max_chunks

    def clear_cache(self, market: str = None) -> bool:
        """ìºì‹œ í´ë¦¬ì–´"""
        if market:
            pattern = f"market_performance:*{market}*"
            return cache_service.clear(pattern)
        else:
            return cache_service.clear()

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        stats = cache_service.get_stats()
        stats.update({
            "config": {
                "chunk_size": self.chunk_size,
                "timeout": self.timeout,
                "max_tickers": self.max_tickers,
                "max_chunks": self.max_chunks,
                "cache_ttl": self.cache_ttl
            }
        })
        return stats

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
PerformanceService.get_market_performance_cached = PerformanceService.get_market_performance